{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "pd.options.display.max_rows = 5\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cv(df,model):   \n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)    \n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "    max_features = ['auto', 'log2','sqrt']\n",
    "\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs = -1)\n",
    "    \n",
    "    rf_random.fit(train_features,train_labels)\n",
    "    return rf_random.best_params_\n",
    "\n",
    "def gridSearch(param_grid,model):\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = rf, \n",
    "                               param_grid = param_grid, \n",
    "                               cv = 5, \n",
    "                               n_jobs = -1, \n",
    "                               verbose = 2)\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_data(df, *argv):\n",
    "    df1 = df.copy()\n",
    "    for series in argv:\n",
    "        df1[series] = df1[series].str.lower()\n",
    "        df1[series] = df1[series].str.replace(\"\\n\",\"\")\n",
    "        df1[series] = df1[series].str.replace(\"\\r\",\"\")\n",
    "        df1[series] = df1[series].str.replace(\"-\",\" \")\n",
    "        df1[series] = df1[series].str.replace(\"[^\\w\\s]\",\"\")\n",
    "        df1[series] = df1[series].str.strip()\n",
    "    df1 = df1.fillna(\"None\")\n",
    "    df1 = df1.replace(\"\", \"missing\")\n",
    "    return df1\n",
    "\n",
    "def explode(df, lst_cols, fill_value=''):\n",
    "    # make sure `lst_cols` is a list\n",
    "    if lst_cols and not isinstance(lst_cols, list):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "\n",
    "    if (lens > 0).all():\n",
    "        # ALL lists in cells aren't empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, lens)\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .loc[:, df.columns]\n",
    "    else:\n",
    "        # at least one list in cells is empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, lens)\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n",
    "          .loc[:, df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = pd.read_csv(\"crunchy_home.csv\")\n",
    "\n",
    "home[\"similar\"] = home[\"similar\"].apply(lambda x: x.split(\"::\"))\n",
    "home[\"tags\"] = home[\"tags\"].apply(lambda x: x.split(\"::\"))\n",
    "home[\"agg_rating\"] = ((home[\"1\"] + home[\"2\"] * 2 + home[\"3\"] * 3 + home[\"4\"] * 4 + home[\"5\"] * 5)/\n",
    "                        home[[\"1\",\"2\",\"3\",\"4\",\"5\"]].sum(axis = 1))\n",
    "\n",
    "home[\"name\"] = home[\"name\"].str.replace(\"-\",\" \").str.lower()\n",
    "\n",
    "reviews = pd.read_csv(\"crunchy_review.csv\")\n",
    "\n",
    "main = pd.read_csv(\"crunchy_main.csv\")\n",
    "\n",
    "main[\"similar\"] = main[\"similar\"].apply(lambda x: x.split(\"::\"))\n",
    "main[\"tags\"] = main[\"tags\"].apply(lambda x: x.split(\"::\"))\n",
    "main[\"name\"] = main[\"name\"].str.replace(\"-\",\" \").str.lower()\n",
    "main[\"agg_rating\"] = ((main[\"1\"] + main[\"2\"] * 2 + main[\"3\"] * 3 + main[\"4\"] * 4 + main[\"5\"] * 5)/\n",
    "                        main[[\"1\",\"2\",\"3\",\"4\",\"5\"]].sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "animelist = clean_data(pd.read_csv(\"/Volumes/SD_Card/myanimelist/AnimeList.csv\"),\"title\", \"title_english\")\n",
    "animelist = animelist[animelist[\"type\"] == \"TV\"]\n",
    "\n",
    "animelist[\"from\"] = animelist[\"aired\"].apply(lambda x : dict(eval(x))).apply(pd.Series)[\"from\"]\n",
    "animelist[\"to\"] = animelist[\"aired\"].apply(lambda x : dict(eval(x))).apply(pd.Series)[\"to\"]\n",
    "\n",
    "merge_anime  = main.merge(animelist[[\"title_english\",\n",
    "                                     \"title\",\"rating\",\n",
    "                                     \"duration\",\"from\",\n",
    "                                     \"to\",\"genre\"]],how = \"inner\", left_on=\"name\",\n",
    "                          right_on=\"title_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_anime = merge_anime[merge_anime[\"duration\"] != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_anime_sample = merge_anime.sample(frac = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = explode(merge_anime_sample,\n",
    "                        \"similar\").drop([\"1\",\"2\",\"3\",\"4\",\"5\",\"agg_review\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"datetime\"] = training_data[\"datetime\"]. \\\n",
    "                            str.replace(\"\\n\", \"\"). \\\n",
    "                            str.strip()\n",
    "\n",
    "training_data[\"datetime\"] = pd.to_datetime(training_data['datetime'],\n",
    "                                           format='%b %d, %Y')\n",
    "\n",
    "training_data[\"datetime\"] = (pd.to_datetime(training_data['datetime']).\n",
    "                                             apply(lambda date: date.toordinal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"duration\"] = training_data[\"duration\"].str.split(\" \").apply(lambda x : x[0]).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"tags\"] = training_data[\"tags\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = clean_data(training_data, \"desc\", \"review\", \"summary\",\"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_training = training_data[[\"similar\",\n",
    "                               \"desc\",\n",
    "                               \"tags\",\n",
    "                               \"review\",\n",
    "                               \"summary\",\n",
    "                               \"genre\",\n",
    "                               \"agg_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise(column):\n",
    "    vec = TfidfVectorizer()\n",
    "    scaler = Normalizer()\n",
    "    model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "    pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "    \n",
    "    return (np.mean(-cross_val_score(pipeline, word_x_train[column], word_training[\"agg_rating\"], \n",
    "                    cv=5,scoring = \"neg_mean_squared_error\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = word_x_train.columns\n",
    "columns = pd.DataFrame(columns).rename(index =str, columns = {0:\"columns\"})\n",
    "\n",
    "columns[\"MSE\"] = columns[\"columns\"].apply(stepwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desc</td>\n",
       "      <td>0.055498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tags</td>\n",
       "      <td>0.056205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>genre</td>\n",
       "      <td>0.056614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>similar</td>\n",
       "      <td>0.210392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>review</td>\n",
       "      <td>0.247045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   columns       MSE\n",
       "1     desc  0.055498\n",
       "2     tags  0.056205\n",
       "5    genre  0.056614\n",
       "0  similar  0.210392\n",
       "3   review  0.247045"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns.sort_values(\"MSE\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion:__\n",
    "\n",
    "We can see that Description, Tags, Genre give the best performance, while review and similar are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to include more than just one text column for our model, to do this we will need to apply a seperate TF-IDF vectorizer to each column. To be able to do this we need to use a Feature Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "\n",
    "transformer = FeatureUnion([\n",
    "                ('similar_tfidf', \n",
    "                  Pipeline([('extract_field',\n",
    "                              FunctionTransformer(lambda x: x['similar'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('desc_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['desc'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('tags_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['tags'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('review_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['review'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('summary_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['summary'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('genre_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['genre'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043149746633677474"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_x_train = word_training[[\"similar\",\"desc\",\"tags\",\"review\",\"summary\",\"genre\"]]\n",
    "\n",
    "scaler = Normalizer()\n",
    "model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "pipeline = Pipeline([(\"vectorizer\",transformer),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "np.mean(-cross_val_score(pipeline, word_x_train, word_training[\"agg_rating\"], \n",
    "                cv=5,scoring = \"neg_mean_squared_error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all our text columns gave us a MSE of $0.043$. That is better than any one of single text columns MSE's.\n",
    "\n",
    "However we say that some text columns were lacking, specifically \"similar\" and \"reviews\". Let's try dropping them and seeing if our model improves in performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FeatureUnion([\n",
    "                ('desc_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['desc'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('tags_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['tags'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('genre_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['genre'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04511762190509126"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_x_train = word_training[[\"desc\",\"tags\",\"genre\"]]\n",
    "\n",
    "scaler = Normalizer()\n",
    "model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "pipeline = Pipeline([(\"vectorizer\",transformer),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "np.mean(-cross_val_score(pipeline, word_x_train, word_training[\"agg_rating\"], \n",
    "                cv=5,scoring = \"neg_mean_squared_error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that dropping those two columns did not help our MSE go down, rather it went up to an MSE of about $0.04511$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating different $K$ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our first FeatureUnion model since we know it worked better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FeatureUnion([\n",
    "                ('similar_tfidf', \n",
    "                  Pipeline([('extract_field',\n",
    "                              FunctionTransformer(lambda x: x['similar'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('desc_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['desc'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('tags_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['tags'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('review_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['review'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('summary_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['summary'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('genre_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['genre'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_x_train = word_training[[\"similar\",\"desc\",\"tags\",\"review\",\"summary\",\"genre\"]]\n",
    "\n",
    "scaler = Normalizer()\n",
    "model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "pipeline = Pipeline([(\"vectorizer\",transformer),(\"scaler\",scaler),(\"fit\",model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate what amount of neighbors for the KNeighborsRegressor is optimal. Do better organize this let's create a dataframe of $K$ values we want to test.\n",
    "\n",
    "We will increment in 10's in order to save runtime, and focus in on the mininium that should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = pd.Series(range(5,65,10))\n",
    "ks.index = range(5,65,10)\n",
    "ks = pd.DataFrame(ks).rename(index= str, columns = {0:\"K\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_k_vals(k):\n",
    "    scaler = Normalizer()\n",
    "    model = KNeighborsRegressor(n_neighbors=k,metric=\"euclidean\")\n",
    "    pipeline = Pipeline([(\"vectorizer\",transformer),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "    return np.mean(-cross_val_score(pipeline, word_x_train, word_training[\"agg_rating\"], \n",
    "                cv=8,scoring = \"neg_mean_squared_error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[\"MSE\"] = ks[\"K\"].apply(test_k_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
