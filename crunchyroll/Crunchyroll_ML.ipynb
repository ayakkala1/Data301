{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "pd.options.display.max_rows = 5\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_beer_class(df):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\")\n",
    "    pattern = \"[{}]\".format(remove)\n",
    "\n",
    "    df.description = df.description.str.lower()\n",
    "    df.description = df.description.str.replace(pattern,\"\")\n",
    "    df.description = df.description.str.replace(\"\\d\",\"\")\n",
    "    \n",
    "    df.name = df.name.str.lower()\n",
    "    df.name = df.name.str.replace(pattern,\"\")\n",
    "\n",
    "\n",
    "    beers = [\"light\",\"pale\",\"hop\",\"malt\",\"light\",\"imperial\",\n",
    "        \"sweet\",\"bitter\",\"citrus\",\"dark\",\"ale\",\n",
    "             \"ipa\",\"double\",\"wheat\",\"beer\",\"style\",\n",
    "            \"lager\",\"big\",\"lambic\",\"pilsner\",\n",
    "             \"porter\",\"stout\",\"barleywine\"]\n",
    "    for beer_type in beers:\n",
    "        text = \"{0}[^ ]*\".format(beer_type)\n",
    "        df[beer_type] = df[\"description\"].str.contains(text,\n",
    "                                                       regex=True,\n",
    "                                                       flags=re.IGNORECASE)\n",
    "        df[beer_type] = df[beer_type].astype(int)\n",
    "    bigrams = [(\"wheat\", \"beer\"),(\"wheat\", \"ale\"),(\"wheat\", \"ale\"),\n",
    "               (\"pale\", \"ale\"),(\"brown\", \"ale\"),(\"double\", \"ipa\")\n",
    "              ,(\"noble\", \"hops\"),(\"ipa\", \"brewed\"),(\"west\", \"coast\"),\n",
    "               (\"dry\", \"hopped\"),(\"american\",\"lager\"),\n",
    "              (\"barley\",\"wine\"),(\"pale\",\"lager\"),(\"american\",\"ipa\"),\n",
    "               (\"imperial\",\"ipa\"),(\"imperial\",\"stout\")]\n",
    "    \n",
    "    for beer_type in bigrams:\n",
    "        bi = set([beer_type])\n",
    "        is_bi = bi.issubset\n",
    "        text = \"_\".join(beer_type)\n",
    "        df[text] = [is_bi(l) for l in df.bigrams.values.tolist()]\n",
    "        df[text] = df[text].astype(int)\n",
    "    tri = set([(\"india\",\"pale\", \"ale\")])\n",
    "    is_tri = tri.issubset\n",
    "    df[\"india_pale_ale\"] = ([is_tri(l) for l in df.trigrams.\n",
    "                             values.\n",
    "                             tolist()])\n",
    "    \n",
    "    df[\"india_pale_ale\"] = df[\"india_pale_ale\"].astype(int)\n",
    "    df[\"ipa_name\"] = df[\"name\"].str.contains(\"ipa\")\n",
    "    df[\"ipa_name\"] = df[\"ipa_name\"].astype(int)\n",
    "    df[\"imperial_name\"] = df[\"name\"].str.contains(\"imperial\")\n",
    "    df[\"imperial_name\"] = df[\"imperial_name\"].astype(int)\n",
    "    df[\"wheat_name\"] = df[\"name\"].str.contains(\"wheat\")\n",
    "    df[\"wheat_name\"] = df[\"wheat_name\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def stepwise(df):\n",
    "    MSE_per_col = pd.Series(index = df.columns)\n",
    "    MSE_per_col.drop([\"description\",\"name\",\"glass\",\"bigrams\",\"trigrams\"],\n",
    "                     inplace=True)\n",
    "    for features in df.columns:\n",
    "        if (features not in [\"description\",\"name\",\"glass\",\"bigrams\",\"trigrams\"]):\n",
    "            X_dict = pd.DataFrame(df[features]).to_dict(orient=\"records\")\n",
    "            y = df[output]\n",
    "            MSE_per_col[features] = (np.mean(\n",
    "                -cross_val_score(pipeline, X_dict,\n",
    "                                 y.values.ravel(),\n",
    "                                 cv=10,\n",
    "                                 scoring=\"neg_mean_squared_error\")\n",
    "            ))\n",
    "    return MSE_per_col\n",
    "\n",
    "def eval_models(df,features_list, cv = 10):\n",
    "    MSE_per_col = pd.Series(index= [\"model{0}\".format(i) for i in range(1,len(features_list)+1)])\n",
    "    counter = 1\n",
    "    for features in features_list:\n",
    "        X_dict = pd.DataFrame(df[features]).to_dict(orient=\"records\")\n",
    "        y = df[\"ibu\"]\n",
    "        MSE_per_col[\"model{0}\".format(counter)] = (np.mean(\n",
    "            -cross_val_score(pipeline,\n",
    "                             X_dict,\n",
    "                             y.values.ravel(),\n",
    "                             cv = cv, scoring = \"neg_mean_squared_error\")\n",
    "        ))\n",
    "        counter += 1\n",
    "    return MSE_per_col\n",
    "\n",
    "def random_cv(df,model):   \n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)    \n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "    max_features = ['auto', 'log2','sqrt']\n",
    "\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs = -1)\n",
    "    \n",
    "    rf_random.fit(train_features,train_labels)\n",
    "    return rf_random.best_params_\n",
    "\n",
    "def gridSearch(param_grid,model):\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = rf, \n",
    "                               param_grid = param_grid, \n",
    "                               cv = 5, \n",
    "                               n_jobs = -1, \n",
    "                               verbose = 2)\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc,arg = None):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = ([token for token in tokens if \n",
    "                        token not in stop_words])\n",
    "    # re-create document from filtered tokens\n",
    "    if arg == \"bigrams\":\n",
    "        return get_bigrams(filtered_tokens)\n",
    "    if arg == \"trigrams\":\n",
    "        return get_trigrams(filtered_tokens)\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "def get_trigrams(words):\n",
    "    return list(zip(words[:-2], words[1:-1],words[2:]))\n",
    "\n",
    "def get_bigrams(words):\n",
    "    return list(zip(words[:-2], words[1:-1]))\n",
    "\n",
    "def predict_spam(new_text,k=30):\n",
    "    # Get the TF-IDF vector for the new text.\n",
    "    x_new = vec.transform([new_text])[0, :]\n",
    "    dot = x_new.multiply(X_train).sum(axis=1)\n",
    "    x_new_len = np.sqrt(x_new.multiply(x_new).sum())\n",
    "    denom = x_new_len * X_train_len\n",
    "    cos_similarities = pd.DataFrame(dot / (denom))[0]\n",
    "    classif = y_train[(cos_similarities.\n",
    "                       sort_values(ascending=False)[:k].\n",
    "                       index)].mean()\n",
    "    return classif\n",
    "\n",
    "def test_k_tf(feature,k,do_gram = False):\n",
    "    if do_gram:\n",
    "        vec = TfidfVectorizer(norm=False,\n",
    "                              ngram_range=(2,2),\n",
    "                              stop_words='english')\n",
    "    else:\n",
    "        vec = TfidfVectorizer(norm=False)\n",
    "    y_train = df[\"ibu\"]\n",
    "    scaler = Normalizer()\n",
    "    model = KNeighborsRegressor(n_neighbors=k,\n",
    "                                metric='euclidean')\n",
    "    pipeline = Pipeline([(\"vectorizer\",vec),\n",
    "                         (\"scaler\",scaler),\n",
    "                         (\"fit\",model)])\n",
    "    return(np.mean(\n",
    "            -cross_val_score(pipeline,\n",
    "                             df[feature],\n",
    "                             y_train.values.ravel(), \n",
    "                             cv=5, scoring=\"neg_mean_squared_error\")\n",
    "        ))\n",
    "\n",
    "def clean_data(df, *argv):\n",
    "    df1 = df.copy()\n",
    "    for series in argv:\n",
    "        df1[series] = df1[series].str.lower()\n",
    "        df1[series] = df1[series].str.replace(\"\\n\",\"\")\n",
    "        df1[series] = df1[series].str.replace(\"\\r\",\"\")\n",
    "        df1[series] = df1[series].str.replace(\"-\",\" \")\n",
    "        df1[series] = df1[series].str.replace(\"[^\\w\\s]\",\"\")\n",
    "        df1[series] = df1[series].str.strip()\n",
    "    df1 = df1.fillna(\"None\")\n",
    "    df1 = df1.replace(\"\", \"missing\")\n",
    "    return df1\n",
    "\n",
    "def explode(df, lst_cols, fill_value=''):\n",
    "    # make sure `lst_cols` is a list\n",
    "    if lst_cols and not isinstance(lst_cols, list):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "\n",
    "    if (lens > 0).all():\n",
    "        # ALL lists in cells aren't empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, lens)\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .loc[:, df.columns]\n",
    "    else:\n",
    "        # at least one list in cells is empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, lens)\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n",
    "          .loc[:, df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = pd.read_csv(\"crunchy_home.csv\")\n",
    "\n",
    "home[\"similar\"] = home[\"similar\"].apply(lambda x: x.split(\"::\"))\n",
    "home[\"tags\"] = home[\"tags\"].apply(lambda x: x.split(\"::\"))\n",
    "home[\"agg_rating\"] = ((home[\"1\"] + home[\"2\"] * 2 + home[\"3\"] * 3 + home[\"4\"] * 4 + home[\"5\"] * 5)/\n",
    "                        home[[\"1\",\"2\",\"3\",\"4\",\"5\"]].sum(axis = 1))\n",
    "\n",
    "home[\"name\"] = home[\"name\"].str.replace(\"-\",\" \").str.lower()\n",
    "\n",
    "reviews = pd.read_csv(\"crunchy_review.csv\")\n",
    "\n",
    "main = pd.read_csv(\"crunchy_main.csv\")\n",
    "\n",
    "main[\"similar\"] = main[\"similar\"].apply(lambda x: x.split(\"::\"))\n",
    "main[\"tags\"] = main[\"tags\"].apply(lambda x: x.split(\"::\"))\n",
    "main[\"name\"] = main[\"name\"].str.replace(\"-\",\" \").str.lower()\n",
    "main[\"agg_rating\"] = ((main[\"1\"] + main[\"2\"] * 2 + main[\"3\"] * 3 + main[\"4\"] * 4 + main[\"5\"] * 5)/\n",
    "                        main[[\"1\",\"2\",\"3\",\"4\",\"5\"]].sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "animelist = clean_data(pd.read_csv(\"/Volumes/SD_Card/myanimelist/AnimeList.csv\"),\"title\", \"title_english\")\n",
    "animelist = animelist[animelist[\"type\"] == \"TV\"]\n",
    "\n",
    "animelist[\"from\"] = animelist[\"aired\"].apply(lambda x : dict(eval(x))).apply(pd.Series)[\"from\"]\n",
    "animelist[\"to\"] = animelist[\"aired\"].apply(lambda x : dict(eval(x))).apply(pd.Series)[\"to\"]\n",
    "\n",
    "merge_anime  = main.merge(animelist[[\"title_english\",\n",
    "                                     \"title\",\"rating\",\n",
    "                                     \"duration\",\"from\",\n",
    "                                     \"to\",\"genre\"]],how = \"inner\", left_on=\"name\",\n",
    "                          right_on=\"title_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_anime = merge_anime[merge_anime[\"duration\"] != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_anime_sample = merge_anime.sample(frac = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = explode(merge_anime_sample,\n",
    "                        \"similar\").drop([\"1\",\"2\",\"3\",\"4\",\"5\",\"agg_review\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"datetime\"] = training_data[\"datetime\"]. \\\n",
    "                            str.replace(\"\\n\", \"\"). \\\n",
    "                            str.strip()\n",
    "\n",
    "training_data[\"datetime\"] = pd.to_datetime(training_data['datetime'],\n",
    "                                           format='%b %d, %Y')\n",
    "\n",
    "training_data[\"datetime\"] = (pd.to_datetime(training_data['datetime']).\n",
    "                                             apply(lambda date: date.toordinal()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"duration\"] = training_data[\"duration\"].str.split(\" \").apply(lambda x : x[0]).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"tags\"] = training_data[\"tags\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = clean_data(training_data, \"desc\", \"review\", \"summary\",\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_training = training_data[[\"similar\",\n",
    "                               \"desc\",\n",
    "                               \"tags\",\n",
    "                               \"review\",\n",
    "                               \"summary\",\n",
    "                               \"genre\",\n",
    "                               \"agg_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise(column):\n",
    "    vec = TfidfVectorizer()\n",
    "    scaler = Normalizer()\n",
    "    model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "    pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "    \n",
    "    return (np.mean(-cross_val_score(pipeline, word_x_train[column], word_training[\"agg_rating\"], \n",
    "                    cv=5,scoring = \"neg_mean_squared_error\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = word_x_train.columns\n",
    "columns = pd.DataFrame(columns).rename(index =str, columns = {0:\"columns\"})\n",
    "\n",
    "columns[\"MSE\"] = columns[\"columns\"].apply(stepwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "\n",
    "transformer = FeatureUnion([\n",
    "                ('similar_tfidf', \n",
    "                  Pipeline([('extract_field',\n",
    "                              FunctionTransformer(lambda x: x['similar'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('desc_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['desc'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('tags_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['tags'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('review_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['review'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('summary_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['summary'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())])),\n",
    "                ('genre_tfidf', \n",
    "                  Pipeline([('extract_field', \n",
    "                              FunctionTransformer(lambda x: x['genre'], \n",
    "                                                  validate=False)),\n",
    "                            ('tfidf', \n",
    "                              TfidfVectorizer())]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('similar_tfidf', Pipeline(memory=None,\n",
       "     steps=[('extract_field', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function <lambda> at 0x124be1a60>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate...      token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_x_train = word_training[[\"similar\",\"desc\",\"tags\",\"review\",\"summary\",\"genre\"]]\n",
    "\n",
    "transformer.fit(word_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_vocab = transformer.transformer_list[0][1].steps[1][1].get_feature_names() \n",
    "desc_vocab = transformer.transformer_list[1][1].steps[1][1].get_feature_names()\n",
    "tags_vocab = transformer.transformer_list[2][1].steps[1][1].get_feature_names() \n",
    "review_vocab = transformer.transformer_list[3][1].steps[1][1].get_feature_names()\n",
    "summary_vocab = transformer.transformer_list[4][1].steps[1][1].get_feature_names() \n",
    "genre_vocab = transformer.transformer_list[5][1].steps[1][1].get_feature_names()\n",
    "\n",
    "vocab = similar_vocab + desc_vocab + tags_vocab + review_vocab + summary_vocab + genre_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "model = KNeighborsRegressor(n_neighbors=30,metric=\"euclidean\")\n",
    "pipeline = Pipeline([(\"vectorizer\",transformer),(\"scaler\",scaler),(\"fit\",model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043149746633677474"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(-cross_val_score(pipeline, word_x_train, word_training[\"agg_rating\"], \n",
    "                cv=5,scoring = \"neg_mean_squared_error\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
