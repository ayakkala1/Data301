{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest w/ IBU Kaggle Compest\n",
    "\n",
    "## Author: Anish Yakkala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "pd.options.display.max_rows = 5\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_beer_class(df):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\")\n",
    "    pattern = \"[{}]\".format(remove)\n",
    "\n",
    "    df.description = df.description.str.lower()\n",
    "    df.description = df.description.str.replace(pattern,\"\")\n",
    "    df.description = df.description.str.replace(\"\\d\",\"\")\n",
    "    \n",
    "    df.name = df.name.str.lower()\n",
    "    df.name = df.name.str.replace(pattern,\"\")\n",
    "\n",
    "\n",
    "    beers = [\"light\",\"pale\",\"hop\",\"malt\",\"light\",\"imperial\",\n",
    "        \"sweet\",\"bitter\",\"citrus\",\"dark\",\"ale\",\n",
    "             \"ipa\",\"double\",\"wheat\",\"beer\",\"style\",\n",
    "            \"lager\",\"big\",\"lambic\",\"pilsner\",\n",
    "             \"porter\",\"stout\",\"barleywine\"]\n",
    "    for beer_type in beers:\n",
    "        text = \"{0}[^ ]*\".format(beer_type)\n",
    "        df[beer_type] = df[\"description\"].str.contains(text,\n",
    "                                                       regex=True,\n",
    "                                                       flags=re.IGNORECASE)\n",
    "        df[beer_type] = df[beer_type].astype(int)\n",
    "    bigrams = [(\"wheat\", \"beer\"),(\"wheat\", \"ale\"),(\"wheat\", \"ale\"),\n",
    "               (\"pale\", \"ale\"),(\"brown\", \"ale\"),(\"double\", \"ipa\")\n",
    "              ,(\"noble\", \"hops\"),(\"ipa\", \"brewed\"),(\"west\", \"coast\"),\n",
    "               (\"dry\", \"hopped\"),(\"american\",\"lager\"),\n",
    "              (\"barley\",\"wine\"),(\"pale\",\"lager\"),(\"american\",\"ipa\"),\n",
    "               (\"imperial\",\"ipa\"),(\"imperial\",\"stout\")]\n",
    "    \n",
    "    for beer_type in bigrams:\n",
    "        bi = set([beer_type])\n",
    "        is_bi = bi.issubset\n",
    "        text = \"_\".join(beer_type)\n",
    "        df[text] = [is_bi(l) for l in df.bigrams.values.tolist()]\n",
    "        df[text] = df[text].astype(int)\n",
    "    tri = set([(\"india\",\"pale\", \"ale\")])\n",
    "    is_tri = tri.issubset\n",
    "    df[\"india_pale_ale\"] = ([is_tri(l) for l in df.trigrams.\n",
    "                             values.\n",
    "                             tolist()])\n",
    "    \n",
    "    df[\"india_pale_ale\"] = df[\"india_pale_ale\"].astype(int)\n",
    "    df[\"ipa_name\"] = df[\"name\"].str.contains(\"ipa\")\n",
    "    df[\"ipa_name\"] = df[\"ipa_name\"].astype(int)\n",
    "    df[\"imperial_name\"] = df[\"name\"].str.contains(\"imperial\")\n",
    "    df[\"imperial_name\"] = df[\"imperial_name\"].astype(int)\n",
    "    df[\"wheat_name\"] = df[\"name\"].str.contains(\"wheat\")\n",
    "    df[\"wheat_name\"] = df[\"wheat_name\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def stepwise(df):\n",
    "    MSE_per_col = pd.Series(index = df.columns)\n",
    "    MSE_per_col.drop([\"description\",\"name\",\"glass\",\"bigrams\",\"trigrams\"],\n",
    "                     inplace=True)\n",
    "    for features in df.columns:\n",
    "        if (features not in [\"description\",\"name\",\"glass\",\"bigrams\",\"trigrams\"]):\n",
    "            X_dict = pd.DataFrame(df[features]).to_dict(orient=\"records\")\n",
    "            y = df[output]\n",
    "            MSE_per_col[features] = (np.mean(\n",
    "                -cross_val_score(pipeline, X_dict,\n",
    "                                 y.values.ravel(),\n",
    "                                 cv=10,\n",
    "                                 scoring=\"neg_mean_squared_error\")\n",
    "            ))\n",
    "    return MSE_per_col\n",
    "\n",
    "def eval_models(df,features_list, cv = 10):\n",
    "    MSE_per_col = pd.Series(index= [\"model{0}\".format(i) for i in range(1,len(features_list)+1)])\n",
    "    counter = 1\n",
    "    for features in features_list:\n",
    "        X_dict = pd.DataFrame(df[features]).to_dict(orient=\"records\")\n",
    "        y = df[\"ibu\"]\n",
    "        MSE_per_col[\"model{0}\".format(counter)] = (np.mean(\n",
    "            -cross_val_score(pipeline,\n",
    "                             X_dict,\n",
    "                             y.values.ravel(),\n",
    "                             cv = cv, scoring = \"neg_mean_squared_error\")\n",
    "        ))\n",
    "        counter += 1\n",
    "    return MSE_per_col\n",
    "\n",
    "def random_cv(df,model):   \n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)    \n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "    max_features = ['auto', 'log2','sqrt']\n",
    "\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 100,\n",
    "                                   cv = 3,\n",
    "                                   verbose=2,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs = -1)\n",
    "    \n",
    "    rf_random.fit(train_features,train_labels)\n",
    "    return rf_random.best_params_\n",
    "\n",
    "def gridSearch(param_grid,model):\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    vec.fit(df[model].to_dict(orient=\"records\"))\n",
    "    train_features = vec.transform(df[model].to_dict(orient=\"records\"))\n",
    "    train_labels = df[\"ibu\"]\n",
    "    scaler = QuantileTransformer()\n",
    "    scaler.fit(train_features)\n",
    "    train_features = scaler.transform(train_features)\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = rf, \n",
    "                               param_grid = param_grid, \n",
    "                               cv = 5, \n",
    "                               n_jobs = -1, \n",
    "                               verbose = 2)\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc,arg = None):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = ([token for token in tokens if \n",
    "                        token not in stop_words])\n",
    "    # re-create document from filtered tokens\n",
    "    if arg == \"bigrams\":\n",
    "        return get_bigrams(filtered_tokens)\n",
    "    if arg == \"trigrams\":\n",
    "        return get_trigrams(filtered_tokens)\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "def get_trigrams(words):\n",
    "    return list(zip(words[:-2], words[1:-1],words[2:]))\n",
    "\n",
    "def get_bigrams(words):\n",
    "    return list(zip(words[:-2], words[1:-1]))\n",
    "\n",
    "def predict_spam(new_text,k=30):\n",
    "    # Get the TF-IDF vector for the new text.\n",
    "    x_new = vec.transform([new_text])[0, :]\n",
    "    dot = x_new.multiply(X_train).sum(axis=1)\n",
    "    x_new_len = np.sqrt(x_new.multiply(x_new).sum())\n",
    "    denom = x_new_len * X_train_len\n",
    "    cos_similarities = pd.DataFrame(dot / (denom))[0]\n",
    "    classif = y_train[(cos_similarities.\n",
    "                       sort_values(ascending=False)[:k].\n",
    "                       index)].mean()\n",
    "    return classif\n",
    "\n",
    "def test_k_tf(feature,k,do_gram = False):\n",
    "    if do_gram:\n",
    "        vec = TfidfVectorizer(norm=False,\n",
    "                              ngram_range=(2,2),\n",
    "                              stop_words='english')\n",
    "    else:\n",
    "        vec = TfidfVectorizer(norm=False)\n",
    "    y_train = df[\"ibu\"]\n",
    "    scaler = Normalizer()\n",
    "    model = KNeighborsRegressor(n_neighbors=k,\n",
    "                                metric='euclidean')\n",
    "    pipeline = Pipeline([(\"vectorizer\",vec),\n",
    "                         (\"scaler\",scaler),\n",
    "                         (\"fit\",model)])\n",
    "    return(np.mean(\n",
    "            -cross_val_score(pipeline,\n",
    "                             df[feature],\n",
    "                             y_train.values.ravel(), \n",
    "                             cv=5, scoring=\"neg_mean_squared_error\")\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ramanyakkala/Downloads/beer_train.csv\")\n",
    "df_test = pd.read_csv(\"/Users/ramanyakkala/Downloads/beer_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = df[\"description\"].fillna(\"Missing\")\n",
    "df_test[\"description\"] = df_test[\"description\"].fillna(\"Missing\")\n",
    "\n",
    "df[\"name\"] = df[\"name\"].fillna(\"Missing\")\n",
    "df_test[\"name\"] = df_test[\"name\"].fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bigrams\"] = (df[\"description\"].\n",
    "                 apply(lambda x: normalize_document(x,\n",
    "                                                    arg=\"bigrams\")))\n",
    "df[\"trigrams\"] = (df[\"description\"].\n",
    "                  apply(lambda x: normalize_document(x,\n",
    "                                                     arg=\"trigrams\")))\n",
    "\n",
    "df_test[\"bigrams\"] = (df_test[\"description\"].\n",
    "                      apply(lambda x: normalize_document(x,\n",
    "                                                         arg=\"bigrams\")))\n",
    "df_test[\"trigrams\"] = (df_test[\"description\"].\n",
    "                       apply(lambda x: normalize_document(x,\n",
    "                                                          arg=\"trigrams\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_beer_class(df)\n",
    "add_beer_class(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(norm=False)\n",
    "vec.fit(df[\"description\"])\n",
    "X_train = vec.transform(df[\"description\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df[\"predicted_ibu\"] = df[\"description\"].apply(lambda x: \n",
    "                                              predict_spam(x,30))\n",
    "\n",
    "vec = TfidfVectorizer(norm=False)\n",
    "vec.fit(df[\"name\"])\n",
    "X_train = vec.transform(df[\"name\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df[\"predicted_ibu_name\"] = df[\"name\"].apply(lambda x: \n",
    "                                            predict_spam(x,30))\n",
    "\n",
    "vec = TfidfVectorizer(norm=False,ngram_range=(2,2))\n",
    "vec.fit(df[\"description\"])\n",
    "X_train = vec.transform(df[\"description\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df[\"predicted_ibu_ngram\"] = df[\"description\"].apply(lambda x: \n",
    "                                                    predict_spam(x,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ibu', 'originalGravity', 'predicted_ibu', 'predicted_ibu_ngram', 'ipa',\n",
       "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
       "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
       "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
       "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
       "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
       "       'imperial_stout', 'barley_wine', 'american_ipa', 'sweet', 'ale', 'pale',\n",
       "       'pilsner', 'noble_hops', 'brown_ale', 'porter', 'stout',\n",
       "       'american_lager', 'lambic', 'pale_lager', 'dark', 'malt', 'isOrganic',\n",
       "       'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse = False)\n",
    "scaler = QuantileTransformer()\n",
    "model = RandomForestRegressor(n_estimators = 100)\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),\n",
    "                     (\"scaler\",scaler),\n",
    "                     (\"fit\",model)])\n",
    "\n",
    "output = [\"ibu\"]\n",
    "\n",
    "stepwises = stepwise(df).sort_values()\n",
    "\n",
    "stepwises.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model3    381.544940\n",
       "model2    383.751835\n",
       "model4    383.955754\n",
       "model1    395.592970\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = QuantileTransformer()\n",
    "output = [\"ibu\"]\n",
    "\n",
    "model8 = ['originalGravity','predicted_ibu_ngram',\n",
    "          'predicted_ibu_name', 'predicted_ibu', 'ipa', 'ipa_name', 'abv', 'srm',\n",
    "          'double', 'hop', 'imperial_name', 'imperial', 'wheat', 'wheat_name',\n",
    "          'light', 'citrus', 'big', 'lager', 'bitter', 'style', 'beer', 'sweet', 'ale', 'pale']\n",
    "\n",
    "model9 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram', 'ipa',\n",
    "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
    "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
    "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
    "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
    "       'imperial_stout', 'barley_wine']\n",
    "\n",
    "\n",
    "model11 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram', 'ipa',\n",
    "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
    "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
    "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
    "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
    "       'imperial_stout', 'barley_wine', 'american_ipa', 'sweet', 'ale', 'pale',\n",
    "       'pilsner', 'noble_hops', 'brown_ale'] \n",
    "\n",
    "model12 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram', 'ipa',\n",
    "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
    "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
    "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
    "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
    "       'imperial_stout', 'barley_wine', 'american_ipa', 'sweet', 'ale', 'pale',\n",
    "       'pilsner', 'noble_hops', 'brown_ale', 'porter', 'stout',\n",
    "       'american_lager']\n",
    "\n",
    "features_list = [model8,model9,model11,model12]\n",
    "\n",
    "eval_model = eval_models(df,features_list)\n",
    "\n",
    "eval_model.sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375.8438802395352\n",
      "375.8438802395352\n",
      "428.63465493584846\n",
      "379.91109906940176\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse=False)\n",
    "scaler = QuantileTransformer()\n",
    "model = KNeighborsRegressor(n_neighbors=40,metric='manhattan')\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "model8 = ['originalGravity', 'predicted_ibu', \n",
    "          'predicted_ibu_ngram', 'ipa',\n",
    "          'abv', 'ipa_name', 'predicted_ibu_name']\n",
    "\n",
    "model9 = ['originalGravity', 'ipa', 'predicted_ibu',\n",
    "          'ipa_name', 'abv',\n",
    "       '  predicted_ibu_name','predicted_ibu_ngram']\n",
    "\n",
    "\n",
    "model10 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram',\n",
    "           'ipa','abv', 'ipa_name', 'predicted_ibu_name',\n",
    "           'srm', 'double', 'hop','double_ipa', 'imperial',\n",
    "           'imperial_ipa']\n",
    "\n",
    "model11 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram',\n",
    "           'ipa','predicted_ibu_name', 'abv',\n",
    "           'ipa_name', 'double']\n",
    "\n",
    "features_list = [model8,model9,model10,model11]\n",
    "output = [\"ibu\"]\n",
    "\n",
    "for features in features_list:\n",
    "    X_dict = pd.DataFrame(df[features]).to_dict(orient=\"records\")\n",
    "    y = df[output]\n",
    "    print((np.mean(\n",
    "        -cross_val_score(pipeline, X_dict, y.values.ravel(), cv=10,\n",
    "                         scoring=\"neg_mean_squared_error\")\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['originalGravity', 'ipa', 'predicted_ibu', \n",
    "            'ipa_name', 'abv',\n",
    "            'predicted_ibu_name','predicted_ibu_ngram']\n",
    "\n",
    "output = [\"ibu\"]\n",
    "\n",
    "X_train_dict = df[features].to_dict(orient=\"records\")\n",
    "X_new_dict = df[features].to_dict(orient=\"records\")\n",
    "y_train = df[output]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "X_new = vec.transform(X_new_dict)\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_new_sc = scaler.transform(X_new)\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=40,metric='manhattan')\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "df[\"KNN_predict\"] = model.predict(X_new_sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model2    375.791049\n",
       "model1    392.268934\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse = False)\n",
    "scaler = QuantileTransformer()\n",
    "model = RandomForestRegressor(n_estimators = 100)\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "output = [\"ibu\"]\n",
    "\n",
    "model7 = ['originalGravity', 'predicted_ibu', 'predicted_ibu_ngram', 'ipa',\n",
    "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
    "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
    "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
    "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
    "       'imperial_stout', 'barley_wine', 'american_ipa', 'sweet', 'ale', 'pale',\n",
    "       'pilsner', 'noble_hops', 'brown_ale'] \n",
    "\n",
    "model8 = ['originalGravity', 'predicted_ibu','KNN_predict', 'predicted_ibu_ngram', 'ipa',\n",
    "       'abv', 'ipa_name', 'predicted_ibu_name', 'srm', 'double', 'hop',\n",
    "       'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale', 'wheat',\n",
    "       'citrus', 'light', 'big', 'lager', 'imperial_name', 'available',\n",
    "       'ipa_brewed', 'wheat_beer', 'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "       'dry_hopped', 'west_coast', 'beer', 'wheat_ale', 'barleywine',\n",
    "       'imperial_stout', 'barley_wine', 'american_ipa', 'sweet', 'ale', 'pale',\n",
    "       'pilsner', 'noble_hops', 'brown_ale'] \n",
    "\n",
    "\n",
    "features_list = [model7,model8]\n",
    "\n",
    "eval_model = eval_models(df,features_list)\n",
    "\n",
    "eval_model.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 8 has performed the best\n",
    "\n",
    "Now I'll do random cv to find better values for our hyper paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = ['originalGravity', 'predicted_ibu','KNN_predict',\n",
    "          'predicted_ibu_ngram',\n",
    "          'ipa','abv', 'ipa_name', 'predicted_ibu_name',\n",
    "          'srm', 'double', 'hop',\n",
    "          'double_ipa', 'imperial', 'imperial_ipa', 'india_pale_ale',\n",
    "          'wheat','citrus', 'light',\n",
    "          'big', 'lager', 'imperial_name',\n",
    "          'available','ipa_brewed', 'wheat_beer',\n",
    "          'wheat_name', 'bitter', 'pale_ale', 'style',\n",
    "          'dry_hopped', 'west_coast', 'beer',\n",
    "          'wheat_ale', 'barleywine','imperial_stout',\n",
    "          'barley_wine', 'american_ipa', 'sweet',\n",
    "          'ale', 'pale','pilsner',\n",
    "          'noble_hops', 'brown_ale'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 43.7min finished\n"
     ]
    }
   ],
   "source": [
    "best_params = random_cv(df,model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1400,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1    354.632622\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse = False)\n",
    "scaler = QuantileTransformer()\n",
    "model = RandomForestRegressor(n_estimators=1400,\n",
    "                              min_samples_split= 5,\n",
    "                              min_samples_leaf= 1,\n",
    "                              max_features= 'sqrt',\n",
    "                              max_depth= None,\n",
    "                             bootstrap= True)\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "eval_models(df,[model8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do grid search and focus in on what values will be perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed: 27.5min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': [1300,1400,1500],\n",
    "                              'min_samples_split': [4,5,6],\n",
    "                              'min_samples_leaf': [1,2,3],\n",
    "                              'max_features': ['sqrt'],\n",
    "                              'max_depth': [None,5,10],\n",
    "                              'bootstrap': [True]}\n",
    "grid = gridSearch(param_grid,model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 1400}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1    354.074357\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse = False)\n",
    "scaler = StandardScaler()\n",
    "model = RandomForestRegressor(n_estimators=1400,\n",
    "                              min_samples_split= 6,\n",
    "                              min_samples_leaf= 2,\n",
    "                              max_features= 'sqrt',\n",
    "                              max_depth= None,\n",
    "                             bootstrap= True)\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),(\"scaler\",scaler),(\"fit\",model)])\n",
    "\n",
    "eval_models(df,[model8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to find the best scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1    354.216207\n",
      "dtype: float64\n",
      "model1    354.974134\n",
      "dtype: float64\n",
      "model1    354.398881\n",
      "dtype: float64\n",
      "model1    354.625004\n",
      "dtype: float64\n",
      "model1    354.485805\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse = False)\n",
    "scalers = [StandardScaler(), \n",
    "           MinMaxScaler(),\n",
    "           RobustScaler(quantile_range=(25, 75)),\n",
    "           MaxAbsScaler(),\n",
    "           QuantileTransformer()]\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1400,\n",
    "                              min_samples_split= 6,\n",
    "                              min_samples_leaf= 2,\n",
    "                              max_features= 'sqrt',\n",
    "                              max_depth= None,\n",
    "                             bootstrap= True)\n",
    "pipeline = Pipeline([(\"vectorizer\",vec),\n",
    "                     (\"scaler\",scaler),\n",
    "                     (\"fit\",model)])\n",
    "\n",
    "for scaler in scalers:\n",
    "    pipeline = Pipeline([(\"vectorizer\",vec),\n",
    "                         (\"scaler\",scaler),\n",
    "                         (\"fit\",model)])\n",
    "    print(eval_models(df,[model8]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add the remaning features to our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(norm=False)\n",
    "vec.fit(df[\"description\"])\n",
    "X_train = vec.transform(df[\"description\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df_test[\"predicted_ibu\"] = (df_test[\"description\"].\n",
    "                            apply(predict_spam))\n",
    "\n",
    "vec = TfidfVectorizer(norm=False,ngram_range=(2,2))\n",
    "vec.fit(df[\"description\"])\n",
    "X_train = vec.transform(df[\"description\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df_test[\"predicted_ibu_ngram\"] = (df_test[\"description\"].\n",
    "                                  apply(predict_spam))\n",
    "\n",
    "vec = TfidfVectorizer(norm=False)\n",
    "vec.fit(df[\"name\"])\n",
    "X_train = vec.transform(df[\"name\"])\n",
    "X_train_len = np.sqrt(X_train.multiply(X_train).sum(axis=1))\n",
    "y_train = df[\"ibu\"]\n",
    "df_test[\"predicted_ibu_name\"] = (df_test[\"name\"].\n",
    "                                 apply(predict_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['originalGravity', 'ipa', 'predicted_ibu',\n",
    "            'ipa_name', 'abv','predicted_ibu_name']\n",
    "\n",
    "\n",
    "output = [\"ibu\"]\n",
    "\n",
    "X_train_dict = df[features].to_dict(orient=\"records\")\n",
    "X_new_dict = df_test[features].to_dict(orient=\"records\")\n",
    "y_train = df[output]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "X_new = vec.transform(X_new_dict)\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_new_sc = scaler.transform(X_new)\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=40,metric='manhattan')\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "df_test[\"KNN_predict\"] = model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict = df[model8].to_dict(orient=\"records\")\n",
    "X_new_dict = df_test[model8].to_dict(orient=\"records\")\n",
    "y_train = df[output]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "vec.fit(X_train_dict)\n",
    "X_train = vec.transform(X_train_dict)\n",
    "X_new = vec.transform(X_new_dict)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_new_sc = scaler.transform(X_new)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1400,\n",
    "                              min_samples_split= 6,\n",
    "                              min_samples_leaf= 2,\n",
    "                              max_features= 'sqrt',\n",
    "                              max_depth= None,\n",
    "                             bootstrap= True)\n",
    "\n",
    "model.fit(X_train_sc, y_train.values.ravel())\n",
    "\n",
    "df_output = pd.DataFrame(index=range(1,4754))\n",
    "df_output[\"id\"] = range(6000,10753)\n",
    "df_output[\"ibu\"] = model.predict(X_new_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in my best score to average the score as an ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vals = pd.read_csv(\"average.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ibu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6000</td>\n",
       "      <td>39.359282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6001</td>\n",
       "      <td>38.821305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>10751</td>\n",
       "      <td>20.411814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>10752</td>\n",
       "      <td>54.581749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4753 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        ibu\n",
       "0      6000  39.359282\n",
       "1      6001  38.821305\n",
       "...     ...        ...\n",
       "4751  10751  20.411814\n",
       "4752  10752  54.581749\n",
       "\n",
       "[4753 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vals.index = average_vals.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[\"ibu\"] = (df_output[\"ibu\"] + average_vals[\"ibu\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv(\"submission_nine.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I used Random Forest since I felt I was plateuing with KNeighrest Neighbors. I also hit a wall with random forest and could not break 18.9 . I think if there was something I could have improved it is how I do my feature selection. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
